{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "OkKJzQCWc7lE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import sys \n",
    "import nltk\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Qt9GmuGwc7lJ"
   },
   "outputs": [],
   "source": [
    "# Let's download the complete text of War and Peace (translated into English by Louise and Aylmer Maude)\n",
    "URL='http://www.gutenberg.org/files/2600/2600-0.txt'\n",
    "!wget -O WaP.txt $URL\n",
    "# The exclamation point above is changing the programming language from Python to Bash.\n",
    "# If you don't understand this, don't worry about it for now.\n",
    "# It's possible to download the data using the Python package \"requests\", but it makes things a little messier.\n",
    "# Feel free to ask me more about this at the end of the workshop :)\n",
    "\n",
    "\n",
    "#  Reads ‘WaP.txt’ file \n",
    "wap = open(\"WaP.txt\", \"r\") \n",
    "raw = wap.read()\n",
    "\n",
    "\n",
    "# We just assigned the text of War and Peace to a variable called \"raw\"\n",
    "\n",
    "\n",
    "# Some info on the type of variable and the how long it is\n",
    "print('The variable \"raw\" contains a', type(raw), 'type object.')\n",
    "print('The variable \"raw\" is', len(raw), 'characters long.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SZ4LxMPcc7lM"
   },
   "outputs": [],
   "source": [
    "#We are only interested in analyzing the text of the novel itself, so we need to get rid of the title, puiblication info, table of contents, etc.\n",
    "\n",
    "#To do this, we need to find the point in the text where the actual novel begins and ends.\n",
    "\n",
    "#Let's look at the first 1000 characters. We can do this by printing the characters from 0 - 1000. \n",
    "\n",
    "#Note: \"printing\" just means displaying something on the screen.\n",
    "\n",
    "\n",
    "print(raw[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oyR7oPPn8LDt"
   },
   "outputs": [],
   "source": [
    "print(raw[1001:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ni_JT1wMjXji"
   },
   "outputs": [],
   "source": [
    "print(raw[-20000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pAYImkRgc7lP"
   },
   "outputs": [],
   "source": [
    "#Project Gutenberg ebooks headers and footers have a lot of info about the text we need to cut out.\n",
    "#We have identified the beginning and end of the text we want.\n",
    "#So let's find the index of the start of the text and the end.\n",
    "#(Note that in a Python string, \"\\n\" indicates a new line).\n",
    "\n",
    "print(raw.find(\"BOOK ONE: 1805\\n\\n\\n\\n\\n\\nCHAPTER I\"))\n",
    "print(raw.find(\"End of the Project Gutenberg EBook of War and Peace, by Leo Tolstoy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Id7vTPjEc7lU"
   },
   "outputs": [],
   "source": [
    "#Overwrite Previous Data with Cleaned\n",
    "#######\n",
    "\n",
    "#We previously worked with the variable \"raw\"\n",
    "#The goal is to overwrite that variable with the new, trimmed data\n",
    "\n",
    "\n",
    "#Overwrite raw with the new slice containing only the text using the identified character indexes\n",
    "raw = raw[7257:3208883]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bmQIrNyZNTGb"
   },
   "outputs": [],
   "source": [
    "#Now let's verify that we've done that correctly by checking the first and last several characters of \"raw\":\n",
    "\n",
    "print('beginning:\\n\\n', raw[:200], '\\n\\n\\n\\n\\n')\n",
    "print('end:\\n\\n', raw[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MhzvhJzIc7lY"
   },
   "outputs": [],
   "source": [
    "#Here we normalize the text a bit and make all letters lowercase.\n",
    "#The function we use to do this is the lower() function.\n",
    "#Why do you think we might want to do this?\n",
    "\n",
    "ti_lower = raw.lower()\n",
    "\n",
    "\n",
    "# Let's quickly compare the two texts to make sure that worked.\n",
    "print(raw[100:300], '\\n\\n\\n')\n",
    "print(ti_lower[100:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HXBN8pavc7la",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Break the text into tokens, which are individual word or punctuation forms.\n",
    "tokens = nltk.word_tokenize(ti_lower)\n",
    "\n",
    "# We can also break the text into sentences (which is trickier than you might think!)\n",
    "# This is useful when we want to analyse things like sentence structure or parts of speech in a text. I will send more materials on this after the workshop.\n",
    "sent_tokens = nltk.sent_tokenize(ti_lower)\n",
    "\n",
    "# Let's take a look:\n",
    "print(tokens[100:200])\n",
    "print(sent_tokens[100:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0esv-slhoFg6"
   },
   "outputs": [],
   "source": [
    "# It looks like this text has no spaces between long dashes and adjacent words.\n",
    "# The presence of newline characters (\\n) also seems to messing with the sentence tokenizer.\n",
    "# Here we will use regular expressions to pad the long dashes with white space and remove the newline characters, replacing them with a single space.\n",
    "# Regular expressions are a handy way of specifying patterns in a text that we want to remove, replace, find, etc.\n",
    "\n",
    "fixed_dashes = re.sub(r'—', ' — ', ti_lower)\n",
    "fixed_newlines = re.sub(r'\\n', ' ', fixed_dashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oyMVRRzFpVCo"
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(fixed_newlines)\n",
    "sent_tokens = nltk.sent_tokenize(fixed_newlines)\n",
    "# Note that we are overwriting the variables \"tokens\" and \"sent_tokens\".\n",
    "\n",
    "print(tokens[100:200])\n",
    "print(sent_tokens[100:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mUo0auxNc7lc"
   },
   "outputs": [],
   "source": [
    "ti_text = nltk.Text(tokens)\n",
    "\n",
    "# We can create a concordance around the word \"war\"\n",
    "\n",
    "\n",
    "ti_text.concordance('war')\n",
    "\n",
    "\n",
    "# Based on this concordance, how would you say 'war' (or some other concept or character) is characterized in War and Peace?\n",
    "# Experiment with other words!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5E2uDHR-c7lf"
   },
   "outputs": [],
   "source": [
    "#Frequency distributions\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "#We can calculate frequency distrubutions\n",
    "\n",
    "\n",
    "FreqDist(tokens).most_common()[:25]\n",
    "#Our frequency doesn't work very well since it is mostly punctuation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DfD7loYfc7lh"
   },
   "outputs": [],
   "source": [
    "#Let's start by removing the punctuation\n",
    "\n",
    "#There are a lot of hyphenated words and contractions in War and Peace.\n",
    "#Let's use the regex tokenizer to keep them, while removing all other punctuation.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"(?x)\\w+(?:[-']\\w+)*\")\n",
    "better_tokens = tokenizer.tokenize(fixed_newlines)\n",
    "\n",
    "\n",
    "# Now let's compare\n",
    "print(tokens[:200])\n",
    "print(better_tokens[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "U3O2QI3Oc7lj"
   },
   "outputs": [],
   "source": [
    "#Now let's remove stopwords (words like \"of\", \"the\", \"a\", \"to\", \"and\", etc.)\n",
    "#NLTK comes with a handy list of stopwords in a few languages\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "#We just created a \"set\", which you can just think of as a list of words to scan through\n",
    "\n",
    "filtered_tokens = []\n",
    "\n",
    "for word in better_tokens:\n",
    "    if word not in stopWords:\n",
    "        filtered_tokens.append(word)\n",
    "#This is a loop!\n",
    "#It performs an action (in this case looking at each word in the text and adding them to the list \"filtered_tokens\")\n",
    "#over and over until it reaches some stop criterion (in this case, reaching the end of the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_oK0nLQRBvKN"
   },
   "outputs": [],
   "source": [
    "#Let's see what the same slice of text looks like now\n",
    "print(better_tokens[:200])\n",
    "print(filtered_tokens[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2BDa5fYeC13Z"
   },
   "outputs": [],
   "source": [
    "#You might notice how it starts to look very different from slice that contained all the stop words very fast.\n",
    "#That just shows you how many stop words there are in a text! \n",
    "#Let's look further ahead in the text to get an idea of the difference.\n",
    "#Then we can see how much shorter the text is after removing the stop words.\n",
    "\n",
    "\n",
    "print(better_tokens[600:650])\n",
    "print(filtered_tokens[600:650])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UXJ-7LIqDyh6"
   },
   "outputs": [],
   "source": [
    "bt_length = len(better_tokens)\n",
    "ft_length = len(filtered_tokens)\n",
    "token_diff = bt_length - ft_length\n",
    "percent_stop_words = 100 * (token_diff / bt_length)\n",
    "\n",
    "print('Including stop words, the text was', bt_length, 'tokens long.')\n",
    "print('But after removing the stop words, the text was only', ft_length, 'tokens long.')\n",
    "print('That means there were', token_diff, \"stop words in the original text. That's\", str(percent_stop_words)+'% of the original text!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PRAGq54kc7ll"
   },
   "outputs": [],
   "source": [
    "#Now let's try a word frequency again\n",
    "\n",
    "word_freq = FreqDist(filtered_tokens)\n",
    "word_freq.most_common()[:25]\n",
    "\n",
    " \n",
    "#Nice!\n",
    "#What does this frequency list suggest about the most common themes in the novel?\n",
    "#This is a useful way of looking at thematic trends in the novel, but can you think of ways in which this could be somewhat misleading? What about different word forms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qgCFxquCc7lm"
   },
   "outputs": [],
   "source": [
    "# It's worth noting that sometimes we want to keep stop words for our analysis.\n",
    "# Stop words are very useful in identifying a particular author's style.\n",
    "# They, along with punctuation, are often crucial for tasks like part-of-speech tagging, syntactic parsing, and many others.\n",
    "\n",
    "\n",
    "# NLTK makes graphing basic frequencies very easy with the built-in .plot function\n",
    "\n",
    "word_freq.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TZJZ_LEMc7lo"
   },
   "outputs": [],
   "source": [
    "# We can also see the most common bigrams (sequence of two words) in the text.\n",
    "# We'll use the older list of tokens leaving the stopwords in so the bigrams are intact.\n",
    "ti_bigrams = list(nltk.bigrams(better_tokens))\n",
    "FreqDist(ti_bigrams).most_common()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TyPw-ucF9M-Y"
   },
   "outputs": [],
   "source": [
    "# And now let's try it again with the stop words removed:\n",
    "\n",
    "ti_filt_bigrams = list(nltk.bigrams(filtered_tokens))\n",
    "FreqDist(ti_filt_bigrams).most_common()[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxHYbtc-Ay4q"
   },
   "source": [
    "Comparative analysis:\n",
    "\n",
    "\n",
    "*   Try to think about how you might use these tools to compare different novels.\n",
    "\n",
    "*   How would the bigram frequencies differ from those of Alice in Wonderland?\n",
    "\n",
    "*   Could we compare word frequencies across different Tolstoy novels?\n",
    "\n",
    "*   What if we broke up War and Peace into sections to see how the frequencies change over time?\n",
    "\n",
    "\n",
    "\n",
    "Shortcomings & pitfalls:\n",
    "\n",
    "\n",
    "*   When might this type of analysis be misleading or problematic?\n",
    "\n",
    "*   What problems might we run into if we are dealing with languages other than English?\n",
    "\n",
    "Non-literary analysis:\n",
    "\n",
    "*   How might some of these approaches translate into the social sciences?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9whFBv292F2"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FINAL_NLTK_workshop",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
